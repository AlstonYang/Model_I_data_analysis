{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3ckGCsc59zR"
   },
   "source": [
    "## Import related API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3044,
     "status": "ok",
     "timestamp": 1616838642242,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "DZ7-4Lqg3y5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential, layers, backend\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, TimeDistributed, LeakyReLU, GRU, BatchNormalization\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the upper limit of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20157,
     "status": "ok",
     "timestamp": 1616838659374,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "HovSE_YS9AEO",
    "outputId": "d327c0b4-23c7-4c81-8577-38a0118688d2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 20721,
     "status": "ok",
     "timestamp": 1616838659941,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "GxcSLwVY9FKy"
   },
   "outputs": [],
   "source": [
    "path = \"WeeklyFinalData.csv\"\n",
    "finalData = read(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 20719,
     "status": "ok",
     "timestamp": 1616838659942,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "dkK-XcAq13_c"
   },
   "outputs": [],
   "source": [
    "train = finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20717,
     "status": "ok",
     "timestamp": 1616838659942,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "W0abl5uIau_W"
   },
   "outputs": [],
   "source": [
    "date = train[\"Date\"]\n",
    "train.drop(\"Date\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRozLYRpau_Y"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOMLTo856gQc"
   },
   "source": [
    "### Add lag time as a predicted factor \n",
    "1. Add lag time from 1 to 4 for CCSP (Yangtze River nonferrous metals, China)\n",
    "2. Split the data to Training set & Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 20717,
     "status": "ok",
     "timestamp": 1616838659943,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "dXrI2P985jos"
   },
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=1, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Zj_gcccau_Z"
   },
   "source": [
    "### Min-max scaling \n",
    "The data is scaled to a fixed range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 20714,
     "status": "ok",
     "timestamp": 1616838659943,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "q4tnCc9Ci5-3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJl8fc4StNyV"
   },
   "source": [
    "### Setting the format of the learning graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 20711,
     "status": "ok",
     "timestamp": 1616838659944,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "PcEcx1JujxBF"
   },
   "outputs": [],
   "source": [
    "def show_raw_visualization(data):\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=int(round(data.shape[1]/2,0)), ncols=2, figsize=(15, 5), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        t_data = data.iloc[:,i]\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i % 2],\n",
    "            color=\"black\",\n",
    "            # title=\"Lag:{0}, {1} curve\".format(lag_time+1, data.columns[i])\n",
    "        )\n",
    "    \n",
    "#     fig.set_size_inches(10,15)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data to the specified time lag and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 20702,
     "status": "ok",
     "timestamp": 1616838659944,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "9aCU6PXt7Ydb"
   },
   "outputs": [],
   "source": [
    "def dataTimeLag(time_lag):\n",
    "\n",
    "    print(\"Lag time = {}\".format(time_lag))\n",
    "    ## Time lag\n",
    "    X1_train, Y1_train= buildTrain(train, pastWeek=time_lag, futureWeek=1)\n",
    "\n",
    "    ## Split date to training & test data\n",
    "    X1_training = pd.DataFrame(X1_train[0:int(X1_train.shape[0]*0.8)])\n",
    "    X1_test = pd.DataFrame(X1_train[int(X1_train.shape[0]*0.8):])\n",
    "\n",
    "    Y1_training = pd.DataFrame(Y1_train[0:int(Y1_train.shape[0]*0.8)])\n",
    "    Y1_test = pd.DataFrame(Y1_train[int(Y1_train.shape[0]*0.8):])\n",
    "\n",
    "#     # Normalize\n",
    "#     print(\"---Normalize---\")\n",
    "#     X1_training_scaled = sc.fit_transform(X1_training)\n",
    "#     X1_test_scaled = sc.transform(X1_test)\n",
    "\n",
    "#     Y1_training_scaled = sc.fit_transform(Y1_training)\n",
    "#     Y1_test_scaled = sc.transform(Y1_test)\n",
    "\n",
    "#     # Check the data dimension\n",
    "#     print(\"---Check the data dimension---\")\n",
    "#     print(\"X1_training_scaled:{0}\".format(X1_training_scaled.shape))\n",
    "#     print(\"Y1_training_scaled:{0}\".format(Y1_training_scaled.shape))\n",
    "#     print(\"X1_test_scaled:{0}\".format(X1_test_scaled.shape))\n",
    "#     print(\"Y1_test_scaled:{0}\".format(Y1_test_scaled.shape))\n",
    "\n",
    "    return [X1_training, Y1_training, X1_test, Y1_test]\n",
    "#     return [X1_training_scaled, Y1_training_scaled, X1_test_scaled, Y1_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag time = 4\n",
      "(380, 18)\n",
      "(380, 1)\n",
      "(95, 18)\n",
      "(95, 1)\n"
     ]
    }
   ],
   "source": [
    "data = dataTimeLag(4)\n",
    "\n",
    "for i in data:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqNv5vJyPsBE"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXho5zzj6N9J"
   },
   "source": [
    "### 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 20695,
     "status": "ok",
     "timestamp": 1616838659945,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "BVmQxhxKwuJF"
   },
   "outputs": [],
   "source": [
    "def buildTwoLayerNN(training_data_shape, setting):\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    regressor = Sequential()\n",
    "    L2 = tf.keras.regularizers.L2(setting[4])\n",
    "    regressor.add(Dense(units=setting[1], activation =setting[0], input_dim=training_data_shape, kernel_initializer = setting[2], kernel_regularizer= L2))\n",
    "    regressor.add(Dense(units=1)) \n",
    "\n",
    "    # adam = optimizers.Adam(lr=0.0001,beta_1=0.9,beta_2=0.999, decay=1e-6)\n",
    "\n",
    "    regressor.compile(optimizer=setting[3], loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    # regressor.summary()\n",
    "\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_eva(raw_data, predict_data, price_range):\n",
    "    total_times = raw_data.shape[0]\n",
    "    correct_times = 0\n",
    "    raw_data=raw_data.values\n",
    "    \n",
    "  ##Test\n",
    "#     print(type(raw_data))\n",
    "#     print(type(predict_data))\n",
    "\n",
    "    for i in range(raw_data.shape[0]): \n",
    "       \n",
    "#         print(raw_data[i], predict_data[i])\n",
    "        \n",
    "        if ((raw_data[i]-price_range) <= predict_data[i]) and (predict_data[i] <= (raw_data[i]+price_range)):\n",
    "            correct_times +=1\n",
    "            \n",
    "    return (correct_times/total_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20689,
     "status": "ok",
     "timestamp": 1616838659945,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "ItBqvK4L9-UN"
   },
   "outputs": [],
   "source": [
    "def training_model(data, setting, setting_name):\n",
    "\n",
    "    ## data\n",
    "    ### [X1_training_scaled, Y1_training_scaled, X1_test_scaled, Y1_test_scaled]\n",
    "\n",
    "    ## Training model\n",
    "\n",
    "    #training_data_shape, activation_function, hidden_node, initializer, optimizer\n",
    "    input_sahpe = data[0].shape[1]\n",
    "    regressor = buildTwoLayerNN(input_sahpe, setting[1:])\n",
    "    history = regressor.fit(data[0], data[1], epochs= setting[0], verbose=0)\n",
    "\n",
    "    ## Draw the learning graph\n",
    "    RMSE = [i for i in history.history[\"root_mean_squared_error\"]]\n",
    "    score_data = pd.DataFrame({\"Loss\":history.history[\"loss\"], \"RMSE\":RMSE})\n",
    "\n",
    "    ## Evaluate the model using testing data\n",
    "    # scores = regressor.evaluate(data[2], data[3])  \n",
    "\n",
    "    ##Test\n",
    "    predict = regressor.predict(data[2])\n",
    "    ##Test\n",
    "    score = np.sqrt(mean_squared_error(predict, data[3]))\n",
    "    accuracy = accuracy_eva(data[3], predict, 3000)\n",
    "    \n",
    "    performance = {\n",
    "        \"Epochs\":setting_name[0],\n",
    "        \"Activation function\":setting_name[1],\n",
    "        \"Hidden nodes\":setting_name[2],\n",
    "        \"Initializer\":setting_name[3],\n",
    "        \"Optimizer\":setting_name[4],\n",
    "        \"Regularizer\":setting_name[5],\n",
    "        \"In-sample RMSE\": RMSE[-1],\n",
    "        \"Out-of-sample RMSE\": score,\n",
    "        \"Out-of-sample Accuracy\": accuracy\n",
    "      # \"RMSE\":scores[1]\n",
    "    }\n",
    "    # print(\"[Out-of-sample performance] \\nEpochs: %d\\nActivation function: %s\\nHidden nodes: %d\\nOptimizer: %s\\nRMSE: %.3f\" %(setting[0], setting[1], setting[2], setting[4], scores[1]))\n",
    "    # show_raw_visualization(score_data)\n",
    "    # print()\n",
    "\n",
    "    ##Test\n",
    "    predicts = pd.DataFrame(predict)\n",
    "    return performance, predicts.stack().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control model Hyperparameter\n",
    "such as:\n",
    "* input factor dimension\n",
    "* epoch\n",
    "* activation_function\n",
    "* hidden_node\n",
    "* initializer\n",
    "* optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 21140,
     "status": "error",
     "timestamp": 1616838660407,
     "user": {
      "displayName": "楊仁瀚",
      "photoUrl": "",
      "userId": "10638930497464391672"
     },
     "user_tz": -480
    },
    "id": "vxaHHmlcDxkt",
    "outputId": "f3bddc5f-f230-491b-bd9f-10930a74f763"
   },
   "outputs": [],
   "source": [
    "def main(input_data):\n",
    "\n",
    "    ## data, epoch, activation_function, hidden_node, initializer, optimizer\n",
    "    ## epoches, activation_functions, hidden_nodes選項\n",
    "    epoches = [100, 200, 300]\n",
    "    activation_functions = [\"sigmoid\", \"tanh\", \"relu\"]\n",
    "    hidden_nodes = [5, 8, 11]\n",
    "\n",
    "    ##Test\n",
    "#     epoches = [100]\n",
    "#     activation_functions = [\"sigmoid\"]\n",
    "#     hidden_nodes = [5]\n",
    "\n",
    "\n",
    "    ## initializer 選項\n",
    "    small_random = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    Xavier = tf.keras.initializers.GlorotNormal()\n",
    "    initializers = [small_random, Xavier]\n",
    "    initializers_name = [\"small_random\", \"Xavier\"]\n",
    "\n",
    "    # optimizer & learning rate decay\n",
    "    learning_rate_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "      initial_learning_rate = 0.01, \n",
    "      decay_steps = 10\n",
    "    )\n",
    "\n",
    "    # learning_rate_fn = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #   initial_learning_rate = 0.01, \n",
    "    #   decay_steps = 100, \n",
    "    #   decay_rate = 0.05,\n",
    "    # )\n",
    "\n",
    "    SGD_cosine = tf.keras.optimizers.SGD(\n",
    "      learning_rate=learning_rate_fn,\n",
    "      name = \"SGD_with_cosine\"\n",
    "    )\n",
    "\n",
    "    Adam_cosine = tf.keras.optimizers.Adam(\n",
    "      learning_rate=learning_rate_fn,\n",
    "      name = \"Adam_with_cosine\"\n",
    "    )\n",
    "\n",
    "    Momentum = tf.keras.optimizers.SGD(\n",
    "      learning_rate=0.01,\n",
    "      momentum=0.9,\n",
    "      name = \"Momentum\"\n",
    "    )\n",
    "\n",
    "    Mom_cosine = tf.keras.optimizers.SGD(\n",
    "      learning_rate=learning_rate_fn,\n",
    "      momentum=0.9,\n",
    "      name = \"Momentum_with_cosine\"\n",
    "    )\n",
    "\n",
    "    regularizer= [0, 0.001, 0.0001]\n",
    "    \n",
    "    ## optimizers 選項\n",
    "    optimizers = [\"SGD\", SGD_cosine, \"Adam\", Adam_cosine, Momentum, Mom_cosine]\n",
    "    optimizers_name = [\"SGD\", \"SGD_with_cosine\", \"Adam\", \"Adam_with_cosine\", \"Momentum\", \"Momentum_with_cosine\"]\n",
    "\n",
    "    ##Test\n",
    "#     optimizers = [\"SGD\",\"Adam\"]\n",
    "#     optimizers_name = [\"SGD\",\"Adam\"]\n",
    "\n",
    "    ##所有hyperparameter的組合\n",
    "    sets = list(product(epoches, activation_functions, hidden_nodes, initializers, optimizers, regularizer))\n",
    "    sets_name = list(product(epoches, activation_functions, hidden_nodes, initializers_name, optimizers_name, regularizer))\n",
    "\n",
    "    data = input_data\n",
    "    RMSE_scores = pd.DataFrame(columns = [\"Epochs\", \"Activation function\", \"Hidden nodes\", \"Optimizer\", \"Initializer\",\"Regularizer\",\"In-sample RMSE\",\"Out-of-sample RMSE\", \"Out-of-sample Accuracy\"])\n",
    "    predicts = pd.DataFrame(columns = list(range(94)))\n",
    "    \n",
    "    for i in range(len(sets)):\n",
    "        \n",
    "        performance, predict = training_model(data, sets[i], sets_name[i])\n",
    "        \n",
    "        RMSE_scores = RMSE_scores.append(performance, ignore_index=True)\n",
    "        predicts = predicts.append(predict, ignore_index=True)\n",
    "\n",
    "    return RMSE_scores, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag time = 4\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f064c4e4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f064c4e42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a72fbad1ca3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataTimeLag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mRMSE_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-a3cd7ca74361>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msets_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mRMSE_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSE_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7b753ba3d551>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(data, setting, setting_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m##Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_eva\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \"\"\"\n\u001b[1;32m    335\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 336\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[1;32m     88\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 664\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n\u001b[0;32m--> 106\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    107\u001b[0m             )\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = dataTimeLag(4)\n",
    "    RMSE_scores, predicts = main(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best top five models based on in-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_scores.sort_values(\"In-sample RMSE\", inplace=True)\n",
    "RMSE_scores.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble the top 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five_model = predicts.iloc[list(RMSE_scores.head(5).index),:]\n",
    "ensemble_predict = top_five_model.apply(lambda x: x.mean())\n",
    "ensemble_RMSE = np.sqrt(mean_squared_error(ensemble_predict, data[3]))\n",
    "print(\"RMSE of ensembling the top 5 models: %.5f\" %ensemble_RMSE)\n",
    "\n",
    "plt.scatter(range(data[3].shape[0]), data[3], color='black', label = 'Raw data')\n",
    "plt.plot(ensemble_predict, label = 'Predicted value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all the results of model configuration in in-sample data as Settings_RMSE_scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_scores.reset_index(inplace=True, drop=True)\n",
    "RMSE_scores.to_csv(\"Settings_RMSE_scores.csv\", index=False)\n",
    "RMSE_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best configuration in out-of-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_scores.sort_values(\"Out-of-sample RMSE\", inplace=True)\n",
    "RMSE_scores.reset_index(inplace=True, drop=True)\n",
    "RMSE_scores[RMSE_scores[\"Out-of-sample RMSE\"]==min(RMSE_scores[\"Out-of-sample RMSE\"])]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
